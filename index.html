<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="A Deep-Dive into the Vision of Vision Language Models" />
    <meta property="og:title" content="What's in the image?" />
    <meta
      property="og:description"
      content="A Deep-Dive into the Vision of Vision Language Models"
    />
    <meta property="og:url" content="https://vision-of-vlm.github.io/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="What's in the image?" />
    <meta
      name="twitter:description"
      content="A Deep-Dive into the Vision of Vision Language Models"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="keywords" content="VLM, Vision-Language Models, Interpretability" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      What’s in the Image? A Deep-Dive into the Vision of Vision Language Models
    </title>
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                What’s in the Image? A Deep-Dive into the Vision of Vision
                Language Models
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://omrikaduri.github.io/" target="_blank"
                    >Omri Kaduri</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://www.weizmann.ac.il/math/bagon/"
                    target="_blank"
                    >Shai Bagon</a
                  ><sup>*</sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://www.weizmann.ac.il/math/dekel/home"
                    target="_blank"
                    >Tali Dekel</a
                  >
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">Weizmann Institute of Science</span>
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Indicates Equal Contribution</small
                  ></span
                >
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                    <a
                      href="static/pdfs/supplementary_material.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link
                  <span class="link-block">
                    <a
                      href="https://github.com/YOUR REPO HERE"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->
                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>



    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. 
                In this paper,  we conduct a thorough empirical analysis, focusing on attention modules across layers.
                We reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of "describe the image"), is utilized by 
                VLMs to store global image information; we demonstrate that these models generate surprisingly descriptive responses solely from these tokens, without direct access to image tokens.  (ii) Cross-modal information flow is predominantly influenced by the middle layers (approximately 25% of all layers), while early and late layers contribute only marginally. 
                (iii) Fine-grained visual attributes and object details are directly extracted from image tokens in a spatially localized manner, i.e., the generated tokens associated with a specific object or attribute attend strongly to their corresponding regions in the image.  
                We propose novel quantitative evaluation to validate our observations, leveraging real-world complex visual scenes. Finally, we demonstrate the potential of our findings in facilitating efficient visual processing in state-of-the-art VLMs.              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Teaser video-->
    <section class="hero teaser ">
      <div class="container is-four-fifths">
        <div class="hero-body">
          <div class="columns is-centered ">
          <div class="column is-four-fifths">
            <br>
          <h2 class="title is-3 has-text-centered">Visual Information Knockout</h2>
          <div>
          <h2 class="subtitle">
              Our analysis reveals that VLMs compress high-level image information into query text tokens, enabling the model to generate descriptive responses even when direct access to image tokens is blocked. Through knockout experiments, we show that visual information is encoded and retrieved indirectly via the query tokens, highlighting their pivotal role as global image descriptors.
          </h2>
          </div>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/banner_video.mp4" type="video/mp4" />
          </video>


        </div>
          </div>
          </div>
      </div>
    </section>

    <!-- Here goes llm-as-a-judge animation -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3 has-text-centered">LLM-as-a-judge for Knockout Evaluation</h2>
                            <h2 class="subtitle">
                Automatically counting the number of identified or hallucinated objects in free-text paragraphs is challenging. <br/>
                Therefore, we propose the LLM-as-a-judge evaluation protocol, which allows us to automatically assess object identification and hallucinated objects in generated responses under different knockout configurations.
                </h2>
              <video poster="" id="tree" autoplay controls muted loop height="100%">
                <source src="static/videos/llm-as-a-judge.mp4" type="video/mp4" />
              </video>

            </div>
          </div>
        </div>
      </div>

    <!-- Image carousel -->
    <section class="hero is-small is-white">
      <div class="hero-body">
        <div class="container is-four-fifths">
        <div class="columns is-centered">

          <div class="column  is-four-fifths ">
          <!-- <div id="results-carousel" class="carousel results-carousel"> -->
            <!-- <div class="item"> -->
              <h2 class="title is-3 has-text-centered">Analyzing Visual Information Flow in Vision-Language Models</h2>
              <div class="has-text-justified">
                  <p>
                      The figure below illustrates the flow of visual information in Vision-Language Models (VLMs) through an <strong>attention knockout</strong> analysis:
                  </p>
                  <ul>
                      <li>
                          <strong>(a) Baseline (No Knockout)</strong>: The VLM employs only causal masking, allowing query and generated tokens to access image tokens.
                      </li>
                      <li>
                          To analyze the role of visual information flow, three attention knockout strategies were employed:
                          <ul>
                              <li><strong>(b) Image-to-generated knockout</strong>: Image tokens influence generated tokens only indirectly via query tokens.</li>
                              <li><strong>(c) Image-to-query knockout</strong>: Query tokens are prevented from accessing image information, isolating them from the visual context.</li>
                              <li><strong>(d) Image-to-others knockout</strong>: Image tokens are blocked from attending to all other tokens.</li>
                          </ul>
                      </li>
                      <li>
                          <strong>(e) Evaluation</strong>: The performance of the model under each knockout configuration (at all layers) was measured. In the Image-to-generated configuration, the model achieved an F1 score of 0.4, demonstrating that query tokens successfully encode and relay global visual information. In contrast, the Image-to-query configuration led to a complete failure, highlighting the essential role of query tokens as global image descriptors.
                      </li>
                      <li>
                          <strong>(f) Knockout from layer onward</strong>: The analysis was extended by applying knockouts starting at different layers. Results indicate a significant increase in F1 scores at the mid-layers, underscoring their critical role.
                      </li>
                  </ul>
                  <p>
                      This analysis highlights the importance of query tokens in encoding global visual information and emphasizes the mid-layers' pivotal role in visual information flow.
                  </p>
              </div>
              <br/>


              <img src="static/images/knonckout_illustration.png" alt="MY ALT TEXT" />
            </div>
          </div>
          </div>
      </div>
    </section>
    <!-- End image carousel -->

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container is-four-fifths">
        <div class="columns is-centered">

          <div class="column  is-four-fifths ">
          <h2 class="title is-3 has-text-centered">Details localized in the mid-layers</h2>
          <div>
              <p>
                  The figure below visualizes the alignment between generated token attention to image tokens and object locations.
              </p>
              <p>
                  Pseudo ground truth object masks were obtained using SAM to validate the alignment. 
                  The peak of the attention maps, marked with a white cross, consistently matches the location of the objects in the image, demonstrating the model's ability to attend to specific visual elements. 
              </p>
          </div>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/attn_to_details-projectpage-1.png" alt="MY ALT TEXT" />
            </div>
            <div class="item">
              <img src="static/images/attn_to_details-projectpage-2.png" alt="MY ALT TEXT" />
            </div>
              <div class="item">
              <img src="static/images/attn_to_details-projectpage-3.png" alt="MY ALT TEXT" />
            </div>
          </div>
        </div>
        </div>
      </div>
    </section>

    <section class="hero teaser is-white ">
      <div class="container is-four-fifths">
        <div class="hero-body">
        <div class="container is-four-fifths">
        <div class="columns is-centered">

          <div class="column  is-four-fifths ">

          <div>
            <h2 class="title is-3 has-text-centered">Image re-prompting</h2>
            <h2 class="subtitle">
              Our analysis reveals that VLMs compress visual information into a small subset of highly attended tokens,
               enabling the creation of a <strong>compressed context</strong> consisting of the top-K% of image tokens and query tokens.
                This compressed context facilitates efficient image re-prompting, allowing the model to answer additional questions without re-processing the full image, achieving near-original performance (96% accuracy) while using significantly fewer tokens—just 5% of the image tokens.
            </h2>
            <br/>
            <!-- put the image in center -->
          <div class="columns is-centered">
              <img src="static/images/image_reprompting.png" alt="MY ALT TEXT" />
              </div>
              <br/>
              <h2 class="subtitle">
    The tables summarize the Image re-prompting evaluation across 10 perception tasks from the MME benchmark. Metrics include accuracy (ACC), ACC+ (percentage of questions answered correctly per image), and the number of tokens used for re-prompting.
The Naive model (i.e., full access to all tokens) achieves slightly higher accuracy, but the compressed context with K=5% retains 96% of the performance while using 12x fewer tokens, highlighting its efficiency for image re-prompting.
              </h2>
                        <div class="columns is-centered"></div>
              <img src="static/images/mme.png" alt="Results of our method on MME benchmark" />
              </div>
        </div>
        </div>
      </div>
      </div>
      </section>

    <!--Acknowledgements-->
    <section class="section" id="Acknowledgements">
      <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgements</h2>
        <p>
          The authors would like to thank Mor Geva Pipek, Yossi Gandelsman, and Boaz Nadler for their valuable feedback.
          This project was supported by an ERC starting grant OmniVideo (10111768).
          Dr Bagon received funding under the MBZUAI-WIS Joint Program for AI Research.
        </p>
      </div>
    </section>

    <!--BibTex citation -->
    <section class="section is-white" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>BibTex Code Here</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page. 
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Default Statcounter code for VLM interp https://vision-of-vlm.github.io/
    -->
    <script type="text/javascript">
    var sc_project=13064057; 
    var sc_invisible=1; 
    var sc_security="6bee2dbb"; 
    </script>
    <script type="text/javascript"
    src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript><div class="statcounter"><a title="web statistics"
    href="https://statcounter.com/" target="_blank"><img class="statcounter"
    src="https://c.statcounter.com/13064057/0/6bee2dbb/1/" alt="web statistics"
    referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->
  </body>
</html>
